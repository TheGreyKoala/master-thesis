\subsection{Klassifizierung einer Webseite}
    \label{section:solutionDetailsClassificationServiceClassification}
    Dieser Abschnitt geht detaillierter auf die Klassifizierung einer Webseite ein
    und beschreibt einige Aspekte der Umsetzung.

    \paragraph{Ablauf}
    Die Klassifizierung folgt einem Algorithmus,
    der rekursiv die Features einer Seitenklasse abarbeitet und nach Entsprechungen
    auf der Webseite sucht.
    Er ist in Listing \ref{listing:classificationAlg} skizziert.

    \lstinputlisting[
        label=listing:classificationAlg,
        caption=Der Klassifizierungsalgorithmus,
        style=pseudo
    ]{../resources/classification.code}

    Die Klassifizierung nutzt die HTML-Repräsentation einer
    Webseite\footnote{vgl. Kapitel \ref{section:conceptClassificationDataSource}},
    weshalb diese zunächst über ihre \gls{url} bezogen und geparst wird,
    wodurch ein \gls{dom} entsteht.
    Mit diesem können Inhalt, Struktur und Aussehen der Webseite
    dynamisch abgefragt werden \cite{w3c:dom}.

    Anschließend wird die Klasse der Seite ermittelt,
    indem bis zur ersten Übereinstimmung der Selektor jeder bekannten
    Seitenklasse auf die Webseite angewandt wird.

    Danach beginnt die Ermittlung der Features der ermittelten Seitenklasse.
    Dazu ruft der Algorithmus die Funktion \texttt{classify} auf,
    die eine Liste der zu suchenden Features und ein HTML-Element erwartet.
    Letzteres bestimmt den Kontext, in dem die Suche stattfindet\footnote{vgl. Kapitel \ref{section:conceptSupportedSelectors}}.
    Im ersten Schritt ist dies das DOM-Objekt \texttt{document},
    welches das gesamte HTML-Dokument wiederspiegelt \cite[Kapitel 1.4]{w3c:dom}.

    Für jedes Feature durchläuft die Funktion die gleichen Schritte.
    Zunächst sucht sie innerhalb des Kontextobjektes nach allen Elementen,
    die dem Selektor des aktuellen Features entsprechen.
    Falls das Feature ein {\scalarFeature} ist, behält sie allerdings nur den ersten Treffer.
    Die an diesem Punkt erhaltenen Elemente werden mit der Klasse des Features klassifiziert.
    Neben dieser Information wird außerdem der eindeutige Selektor jedes Elementes bestimmt
    und ebenfalls gespeichert\footnote{vgl. Kapitel \ref{section:conceptPageDataModel}}.
    Handelt es sich um ein {\collectionFeature}, ruft sich \texttt{classify} anschließend rekursiv für jedes klassifizierte Element auf.
    In jedem Aufruf verwendet sie als Parameter die {\childFeature}s der Klasse sowie das klassifizierte HTML-Element.

    \paragraph{Browserautomatisierung}
    Die Herausforderung bei der Implementierung dieses Algorithmus
    ist die Auswertung der Selektoren und das dazu notwendige Parsen des \gls{html}-Dokumentes.
    Das \gls{wccs} setzt zur Bewältigung dieser Aufgabe auf die Automatisierung eines Webbrowsers.
    Konkret verwendet es die Bibliothek
    Puppeteer\footnote{\url{https://github.com/GoogleChrome/puppeteer}},
    mit deren Hilfe sich der Browser Google Chrome ohne die Anzeige einer graphischen Oberfläche starten
    und anschließend programmatisch steuern lässt.

    Der wichtigste Vorteil der Nutzung eines echten Browsers ist seine ausgereifte und umfangreiche Implementierung
    als Parser von \gls{html}, CSS und XPath.
    Des Weiteren führt der Webbrowser das auf der Seite enthaltene JavaScript aus,
    wodurch sich auch der Inhalt verändern kann.

    \paragraph{Selektoren}
    Dank Puppeteer kann zur Umsetzung der Selektoren auf standardisierte Schnittstellen
    genutzt werden, die die benötigten Informationen liefern.
    Der {\cssSelector} kann auf die Methode \texttt{querySelectorAll} zurückgreifen,
    die sowohl auf \texttt{document} als auch auf beliebigen \gls{html}-Elementen verfügbar ist
    \cite[Kapitel 6.1]{w3c:selectorsAPI}.
    Diese Methode wertet CSS-Ausdrücke auf der Webseite aus und gibt alle gefundenen Elemente zurück,
    die Kinder des Elementes sind, auf dem sie aufgerufen
    wurde\footnote{vgl. Kapitel \ref{section:conceptSupportedSelectors}}.

    Der {\xpathSelector} wird hingegen über die Methdoe \texttt{evaluate} des Objektes \texttt{document} realisiert.
    % TODO: Ref: https://developer.mozilla.org/en-US/docs/Web/API/Document/evaluate
    Diese Methode erwartet neben dem XPath-Ausdruck auch ein Kontextobjekt,
    sodass auch hier der beschriebene Algorithmus leicht umgesetzt werden kann.
    Das Ergebnis dieser Methode kann abhängig vom XPath-Ausdruck eine Liste von HTML-Elementen, eine Zeichenkette,
    ein boolescher Wert oder eine Zahl sein.
    Die beiden zuletzt genannten werden vom \gls{wccs} nicht unterstützt und als keine Übereinstimmung mit dem Selektor gewertet.
    Bei HTML-Elementen wird der Algorithmus wie beschrieben weiter durchlaufen.
    Im Falle von Zeichenketten wird die Rekursion allerdings unterbrochen, da kein neues Element vorhanden ist,
    welches den Suchraum weiter einschränken könnte.
    Abhängig von der Art des Features wird die Zeichenkette als textueller Inhalt
    oder als \gls{url} einer referenzierten {\resource} interpretiert.

    Der {\urlSelector} ist für Webseiten und Features unterschiedlich implementiert.
    Im Falle von Seiten wird der reguläre Ausdruck auf ihre \glspl{url} angewandt.
    Für Features werden zunächst alle Elemente innerhalb des Kontextobjektes gesucht, die {\resources} referenzieren.
    Das sind diejenigen Elemente, die das Attribute \texttt{href}, \texttt{src} oder \texttt{srcset} besitzen.
    % TODO: Ref: (siehe Mindmap
    Um diese Elmente zu finden wird der {\cssSelector} \texttt{[href], [src], [srcset]} auf dem Kontextobjekt ausgeführt.
    Anschließend wird für jeden Treffer der Wert des gefundenen Attributes ausgelesen
    und der reguläre Ausdruck des {\urlSelector}s auf ihm angewandt.
    Bei einer Übereinstimmung wird das Element in die Ergebnismenge aufgenommen.

    Aus diesen Beschreibungen folgt, dass Selektoren selbst keine Unterscheidung bez.
    der Kardinalität eines Features machen.
    Stattdessen liefern sie immer alle Treffer und überlassen dem Aufrufer die Aufgabe, zu entscheiden,
    welche Elemente er verwendet.
    Das wird auch in Listing \ref{listing:classificationAlg} deutlich.

    \paragraph{Textueller Inhalt oder Ziel eines Features}
    Wie in Kapitel \ref{section:conceptPageDataModel} beschrieben speichert
    ein Content Feature seinen textuellen Inhalt und ein Reference Feature die \gls{url} seines Ziels.
    Bei Content Features gilt allerdings die Einschränkung, dass nur solche Features ihren textuellen Inhalt speichern,
    deren Klasse selbst keine Content Features besitzt.
    Der Grund ist, dass durch Child Features der Text spezieller klassifiziert und in diesen Features
    feingranularer gespeichert werden kann.
    Die entsprechenden Werte für Inhalt und Ziel müssen während der Klassifizierung ermittelt werden.

    Für Text eines Content Features ist die Eigenschaft "`innerText"' des zugehörigen Nodes.
    Anders als "`textContent"' enthält diese Eigenschaft nur den gerenderten Text, d.h. das, was ein Webseitenbesucher sieht.
    % TODO: Ref: https://html.spec.whatwg.org/#the-innertext-idl-attribute
    Die Eigenschaft "`textContent"' enthält auch Kommentare, style und script-Tags oder Zeilenumbrüche des Quelltextes.
    Für den vorliegenden Anwendungsfall ist "`innerText"' deshalb besser geeignet.

    \paragraph{Bestimmung eines eindeutigen Selektors}
    Am Ende der Klassifizierung liegt ein Dokument vor, dass dem Datenmodell aus Kapitel \ref{section:conceptPageDataModel} entspricht.
    Dazu muss für jeden Node ein eindeutiger Selektor ermittelt werden,
    der zwei Anwendungsfälle hat.
    Annotator soll ihn Nutzen, die Knoten hervorzuheben.
    Drittsysteme sollen bei Bedarf den aktuellen Text / Ziel des Knotes ermittelt können.
    Das \gls{wccs} folgt dabei dem Format von Annotator, welches wiederum auf der Selection DOM-API basiert.
    % TODO: Ref: https://developer.mozilla.org/en-US/docs/Web/API/Selection
    Ein Selektor ist demnach ein RangeSelector.
    Die Range besteht zwischen zwei Punkten,
    die über XPath-Selektoren und start und endOffset bestimmt werden.
    Zur Bestimmung der Werte gibt es API, die man nutzen kann, um z. B. einen Node auszuwählen.
    Das ist das was man bräuchte.
    Allerdings geht Annotator anders vor.
    Annotator bestimmt den ersten Text-Node und geht solange immer tiefer
    in die ersten Child-Nodes, bis es einen Text-Node findet.
    Der wird dann ausgewählt.
    Das Ergebnis ist, dass Ranges, die über die API ermittelt wurden,
    nicht mit Annotator kompatibel sind und Bereichne nicht richtig hervorgehoben werden.
    Die Logik von Annotator könnte jetzt nachgebaut werden, aber das wäre semantisch falsch.
    Wir selektieren einen meistens einen ganzen Node und wollen für den den Selektor.
    Nicht für den ersten Text Node, der ggf. erst im Dritten Child Node kommt.
    Das wäre auch für Drittsysteme schwer, die die schwere Logik immer nachbauen müssten.
    Das \gls{wccs} verwendet einen Mittelweg, der beide Fälle gut genug abdeckt.
    Der XPath selektiert immer den konkreten Node und ist meistens für Start und Ende gleich.
    Er folgt dem Algorithmus von Annotator, welcher sich die Vaterkette der Nodes hocharbeitet,
    die Namen der besuchten Nodes durch einen Slash getrennt aneinanderhängt
    und außerdem die Position eines Nodes innerhalb des Parents anhängt.
    Beispiel: /html[1]/body[1]/div[2]/header[1].
    Startoffset ist 0, Endoffset ist die Länge von innerText.
    Das ist leicht erklär- und nachvollziehbar für Drittsysteme und führt bei Annotator meistens zum richtigen Ergebnis.
    Ein Spezialfall ist, wenn über XPath ein String ausgewählt wurde.
    Xpath ist weiterhin der des Nodes, Startoffset ist aber die Startposition des Strings innerhalb des innerTextes des Nodes.
    Endoffset ist startOffset + Länge des Strings.
    Sie markieren also die Position des String innerhalb des innerTexts.
    Das ist immernoch gut für Drittsysteme, führt bei Annotator aber manchmal zu leichten Verschiebungen der Hervorhebung.

    % TODO: Ist das interessant? \paragraph{Limitierung der gleichzeitig ausgeführten Klassifizierungen}
