\subsection{Klassifizierung einer Webseite}
    \label{section:solutionDetailsClassificationServiceClassification}
    Dieser Abschnitt geht detaillierter auf die Klassifizierung einer Webseite ein
    und beschreibt einige Aspekte der Umsetzung.

    \paragraph{Ablauf}
    Die Klassifizierung folgt einem Algorithmus,
    der rekursiv die Features einer Seitenklasse abarbeitet und nach Entsprechungen
    auf einer Webseite sucht.
    Er ist in Listing \ref{listing:classificationAlg} skizziert.

    \lstinputlisting[
        label=listing:classificationAlg,
        caption=Der Klassifizierungsalgorithmus,
        style=pseudo
    ]{../resources/classification.code}

    Die Klassifizierung nutzt die HTML-Repräsentation einer
    Webseite\footnote{vgl. Kapitel \ref{section:conceptClassificationDataSource}},
    weshalb diese zunächst über ihre \gls{url} bezogen und geparst wird,
    wodurch ein \gls{dom} entsteht.
    Mit diesem können Inhalt, Struktur und Aussehen der Webseite
    dynamisch abgefragt werden \cite{w3c:dom}.
    Anschließend wird die Klasse der Seite ermittelt,
    indem bis zur ersten Übereinstimmung der Selektor jeder bekannten
    Seitenklasse auf die Webseite angewandt wird.
    Danach beginnt die Ermittlung der Features der ermittelten Seitenklasse.
    Dazu ruft der Algorithmus die Funktion \texttt{classify} auf,
    die eine Liste der zu suchenden Features und ein HTML-Element erwartet.
    Letzteres bestimmt den Kontext, in dem die Suche stattfindet\footnote{vgl. Kapitel \ref{section:conceptSupportedSelectors}}.
    Im ersten Schritt ist dies das DOM-Objekt \texttt{document},
    welches das gesamte HTML-Dokument widerspiegelt \cite[Kapitel 1.4]{w3c:dom}.
    Für jedes Feature durchläuft die Funktion die gleichen Schritte.
    Zunächst sucht sie innerhalb des Kontextobjektes nach allen Elementen,
    die dem Selektor des aktuellen Features entsprechen.
    Falls das Feature ein {\scalarFeature} ist, behält sie allerdings nur den ersten Treffer.
    Die Klassifikation der Elemente wird anschließend gespeichert.
    Dabei wird sowohl der textuelle Inhalt bzw. die \gls{url}
    der referenzierten {\resource} als auch ein eindeutiger Selektor eines Elementes ermittelt
    und gespeichert\footnote{vgl. Kapitel \ref{section:conceptPageDataModel}}.
    Handelt es sich um ein {\collectionFeature}, ruft sich \texttt{classify} anschließend rekursiv für jedes klassifizierte Element auf.
    In jedem Aufruf verwendet sie als Parameter die {\childFeature}s der Klasse sowie das klassifizierte HTML-Element.

    \paragraph{Browserautomatisierung}
    Die Herausforderung bei der Implementierung dieses Algorithmus
    ist die Auswertung der Selektoren und das dazu notwendige Parsen des \gls{html}-Dokumentes.
    Das \gls{wccs} setzt zur Bewältigung dieser Aufgabe auf die Automatisierung eines Webbrowsers.
    Konkret verwendet es die Bibliothek
    Puppeteer\footnote{vgl. \url{https://github.com/GoogleChrome/puppeteer}},
    mit deren Hilfe sich der Browser Google Chrome ohne die Anzeige einer grafischen Oberfläche starten
    und anschließend programmatisch steuern lässt.
    Der wichtigste Vorteil der Nutzung eines echten Browsers ist seine ausgereifte und umfangreiche Implementierung
    als Parser von \gls{html}, CSS und XPath.
    Des Weiteren führt der Webbrowser das auf der Seite enthaltene JavaScript aus,
    was für die Klassifizierung von Bedeutung sein kann.

    \paragraph{Selektoren}
    Dank Puppeteer kann zur Umsetzung der Selektoren auf standardisierte Schnittstellen
    zurückgegriffen werden, die die benötigten Informationen liefern.
    Der {\cssSelector} kann die Methode \texttt{querySelectorAll} nutzen,
    die sowohl auf \texttt{document} als auch auf beliebigen \gls{html}-Elementen verfügbar ist
    \cite[Kapitel 6.1]{w3c:selectorsAPI}.
    Diese Methode wertet CSS-Ausdrücke auf der Webseite aus und gibt alle gefundenen Elemente zurück,
    die Kinder des Elementes sind, auf dem sie aufgerufen
    wurde\footnote{vgl. Kapitel \ref{section:conceptSupportedSelectors}}.

    Der {\xpathSelector} wird hingegen über die Methode \texttt{evaluate} des Objektes \texttt{document} realisiert
    \cite[Kapitel 1.4]{w3c:domXPath}.
    Diese Methode erwartet neben dem XPath-Ausdruck auch ein Kontextobjekt,
    sodass auch hier der beschriebene Algorithmus leicht umgesetzt werden kann.
    Das Ergebnis dieser Methode kann abhängig vom XPath-Ausdruck eine Liste von HTML-Elementen, eine Zeichenkette,
    ein boolescher Wert oder eine Zahl sein
    \cite[Kapitel 1.4]{w3c:domXPath}.
    Die beiden zuletzt genannten Ergebnisse werden vom \gls{wccs} nicht unterstützt und als keine Übereinstimmung mit dem Selektor gewertet.
    Bei HTML-Elementen wird der Algorithmus wie beschrieben weiter durchlaufen.
    Im Falle von Zeichenketten wird die Rekursion allerdings unterbrochen, da kein neues Element vorhanden ist,
    welches den Suchraum weiter einschränken könnte.
    Abhängig von der Art des Features wird die Zeichenkette als textueller Inhalt
    oder als \gls{url} einer referenzierten {\resource} interpretiert.
    Durch die Verwendung von \texttt{document.evaluate} ist ein {\xpathSelector}
    auf die Möglichkeiten von XPath 1.0 eingeschränkt \cite{w3c:domXPath}.

    Der {\urlSelector} ist für Webseiten und {\referenceFeature}s unterschiedlich implementiert.
    Im Falle von Seiten wird der reguläre Ausdruck auf ihren \glspl{url} angewandt.
    Eine Übereinstimmung wird als Erfüllung des Selektors gewertet.
    Für {\referenceFeature}s werden zunächst alle Elemente innerhalb des Kontextobjektes gesucht, die {\resources} referenzieren.
    Das sind diejenigen Elemente, die eines der Attribute \texttt{href}, \texttt{src} oder \texttt{srcset} besitzen
    \cite[Kapitel 4.7 \& 4.8]{w3c:html5}.
    Um diese Elmente zu finden, wird der {\cssSelector} \texttt{[href], [src], [srcset]} auf dem Kontextobjekt ausgeführt.
    Anschließend wird für jeden Treffer der Wert des gefundenen Attributes ausgelesen
    und der reguläre Ausdruck des {\urlSelector}s auf ihm angewandt.
    Bei einer Übereinstimmung wird das Element in die Ergebnismenge aufgenommen.

    Aus diesen Beschreibungen folgt, dass Selektoren selbst keine Unterscheidung bez.
    der Kardinalität eines Features machen.
    Stattdessen liefern sie immer alle Treffer und überlassen dem Aufrufer die Aufgabe zu entscheiden,
    welche Elemente er verwendet.
    Das wird auch in Listing \ref{listing:classificationAlg} deutlich.

    \paragraph{Textueller Inhalt und Referenzziel eines Features}
    Ein {\contentFeature} speichert seinen textuellen Inhalt und ein {\referenceFeature}
    die \gls{url} seines Zieles\footnote{vgl. Kapitel \ref{section:conceptPageDataModel}}.
    Bei {\contentFeature}s gilt allerdings die Einschränkung, dass nur solche ihren textuellen Inhalt speichern,
    bei denen sich kein {\contentFeature} unter den {\childFeature}s befindet.
    Der Grund ist, dass durch solche {\childFeature}s der Text spezieller klassifiziert und
    feingranularer gespeichert werden kann.
    Inhalt bzw. Ziel müssen während der Klassifizierung ermittelt werden.
    Für {\contentFeature}s nutzt das \gls{wccs} die Eigenschaft \texttt{innerText} des klassifizierten HTML-Elementes.
    Anders als \texttt{textContent} enthält diese Eigenschaft nur den gerenderten Text,
    wodurch \texttt{style}- und \texttt{script}-Elemente sowie Kommentare und Zeilenumbrüche im Quelltext
    ausgeschlossen werden
    \cite[Kapitel 3.2.7]{whatwg:html}.
    Für den Anwendungsfall des \gls{wccs} ist \texttt{innerText} deshalb besser geeignet als \texttt{textContent}.
    Die \gls{url} einer {\resource} bezieht das \gls{wccs} aus den Attributen
    \texttt{href}, \texttt{src} oder \texttt{srcset} des klassifizierten HTML-Elementes.

    \paragraph{Bestimmung eines eindeutigen Selektors}
    Eine vollständige Klassifikation enthält für jedes Feature auch einen eindeutigen
    Selektor des klassifizierten HTML-Elementes\footnote{vgl. Kapitel \ref{section:conceptPageDataModel}}.
    Da diese Information von Annotator benötigt wird,
    um Annotationen zu erzeugen,
    orientiert sich das \gls{wccs} für den Aufbau und das Format eines Selektors
    an dieser Bibliothek.
    Ein Selektor besteht demnach aus vier Elementen
    \cite[Kapitel "`Annotation format"']{annotator:documentation}:
    Zwei XPath-Ausdrücken, die ein Start- und ein Endelement identifizieren
    sowie zwei numerischen Angaben, die Positionen in diesen Elementen festlegen.
    Dadurch kann Annotator eine Annotation an einer beliebigen Stelle eines Elementes beginnen
    und innerhalb eines anderen Elementes beenden.
    Im Falle des \glspl{wccs} sind Start- und Endelement immer identisch,
    da mit den Selektoren des {\classificationModel}s genau ein Element
    oder Text innerhalb eines Elementes erfasst wird.
    Die Versatzangaben bestimmen dabei, welcher Bereich dieses Elementes klassifiziert wurde.
    Mit der Selection API des \glspl{w3c} \cite{w3c:selectionAPI} lassen sich diese Angaben bestimmen,
    was dem \gls{wccs} prinzipiell eine einfache und standardisierte Methode eröffnet.
    Annotator folgt allerdings einem eigenen Algorithmus,
    der nach eigenen Versuchen andere Resultate liefert als die Selection API.
    Selektoren, die mit der Selection API bestimmt wurden,
    setzt Annotator deshalb nicht den Erwartungen entsprechend in Annotationen um.
    Würde das \gls{wccs} bei der Bestimmung eines eindeutigen Selektors Annotators
    Algorithmus folgen, müssten Drittsysteme diesen kennen,
    um die Angaben korrekt interpretieren zu können.
    Das \gls{wccs} verwendet deshalb einen Mittelweg,
    der sowohl gute Ergebnisse in Annotator liefert,
    als auch leicht durch Drittsysteme nachvollzogen werden kann.
    Die XPath-Ausdrücke sind absolute Angaben, die immer dasselbe Element erfassen und aufgrund ihrer Standardisierung
    für jedes Drittsystem eindeutig zu interpretieren sind.
    Das \gls{wccs} folgt bei der Berechnung dieser Ausdrücke Annotators Vorgehen,
    um eine größtmögliche Kompatibilität zu erzielen.
    Ausgehend von einem Element wird der Vaterkette bis zur Wurzel des \gls{html}-Dokumentes
    gefolgt und dabei die Namen und Positionen der besuchten Elemente gesammelt.
    Aus diesen Informationen lässt sich ein eindeutiger XPath-Ausdruck erzeugen.
    Wurde durch den Selektor des {\classificationModel}s ein vollständiges Element klassifiziert,
    ist der Versatz im Startelement 0, im Endelement die Länge der Eigenschaft \texttt{innerText}.
    Das führt bei Annotator meist zum richtigen Ergebnis.
    Falls durch einen {\xpathSelector} eine Zeichenkette in einem Element erfasst wurde,
    berechnen sich Start- und Endversatz wie folgt, wobei die Variable \texttt{text} den klassifizierten Text bezeichnet:

    \begin{lstlisting}
startOffset = node.innerText.indexOf(text)
endOffset = startOffset + text.length
    \end{lstlisting}
    
    Diese Berechnung ist leicht nachzuvollziehen,
    führt bei Annotator in manchen Fällen aber zu leicht verschobenen
    Annotationen\footnote{vgl. Kapitel \ref{section:findingsTeachersAbnormalitiesBabw}}.

    \paragraph{Parallelisierung}
    Mit dem beschriebenen Verfahren können prinzipiell beliebig viele Webseiten parallel klassifiziert werden.
    Bei sehr vielen gleichzeitigen Anfragen dürfen aber nicht alle Seiten
    gleichzeitig über Puppeteer im Browser geöffnet werden.
    Die Anzahl der dadurch erzeugten Prozesse kann zum Einfrieren des Systems führen.
    Aus diesem Grund begrenzt der {\classificationService} die Anzahl der gleichzeitig klassifizierten Webseiten
    und verwaltet eine Warteschlange seiner Aufträge.
    Es ist außerdem möglich, die Klassifizierung einer einzelnen Webseite zu parallelisieren.
    Zwischen Features mit demselben direkten {\parentFeature} besteht nämlich keine Abhängigkeit,
    sodass sie nebenläufig abgearbeitet werden können.
